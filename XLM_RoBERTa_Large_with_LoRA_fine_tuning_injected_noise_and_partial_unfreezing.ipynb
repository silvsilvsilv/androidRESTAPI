{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8aa66e2e26ca4e06b8f9d17e39ca6ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce1b8f0704874b20b483b22924365a99",
              "IPY_MODEL_61294b9a721d44949e1e0a6ac6378364",
              "IPY_MODEL_a8b7f6db03694f788888e1bd3c6b9abc"
            ],
            "layout": "IPY_MODEL_f3fcbac128404fe5ab997cca6e719e47"
          }
        },
        "ce1b8f0704874b20b483b22924365a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ce7d679a8fc41d9b2b1d871bfa5434f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4a89766033bf45278e894d0bbd2110b9",
            "value": "Map:‚Äá100%"
          }
        },
        "61294b9a721d44949e1e0a6ac6378364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd362f1114664dc5834c950b36d7548d",
            "max": 21767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27c7f409b51f489a996a90ab7e13c887",
            "value": 21767
          }
        },
        "a8b7f6db03694f788888e1bd3c6b9abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39ffa8fc49054253aba8decbb744a301",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fd49d890c7494dce932a168fb19218c7",
            "value": "‚Äá21767/21767‚Äá[00:01&lt;00:00,‚Äá12778.64‚Äáexamples/s]"
          }
        },
        "f3fcbac128404fe5ab997cca6e719e47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ce7d679a8fc41d9b2b1d871bfa5434f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a89766033bf45278e894d0bbd2110b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd362f1114664dc5834c950b36d7548d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27c7f409b51f489a996a90ab7e13c887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39ffa8fc49054253aba8decbb744a301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd49d890c7494dce932a168fb19218c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e05667d7d1f14fa4bd042b0a0fcb9b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c941d1c5fb6d4cdd9fbbf4b8e613e1ab",
              "IPY_MODEL_d4714a97f35d4b4389de1210a48498e7",
              "IPY_MODEL_7f7deabc545e498594c7cab4a57bfaed"
            ],
            "layout": "IPY_MODEL_fb6e6f47dc3a40c2b44f9d031392c344"
          }
        },
        "c941d1c5fb6d4cdd9fbbf4b8e613e1ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cd911ef29194f1bbbfda7be7699959b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dce4006d90ad4f38bd1f46a7fb249816",
            "value": "Map:‚Äá100%"
          }
        },
        "d4714a97f35d4b4389de1210a48498e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a6cfe00751426590e946b307374e97",
            "max": 2800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ea065a6e9bb4e4a9801a2399cd8d430",
            "value": 2800
          }
        },
        "7f7deabc545e498594c7cab4a57bfaed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee8a2bf38bb4a0eb3dfb7ed5f9e8565",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b7e48f9232c542a083af60e7d0e35098",
            "value": "‚Äá2800/2800‚Äá[00:00&lt;00:00,‚Äá11940.11‚Äáexamples/s]"
          }
        },
        "fb6e6f47dc3a40c2b44f9d031392c344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cd911ef29194f1bbbfda7be7699959b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce4006d90ad4f38bd1f46a7fb249816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17a6cfe00751426590e946b307374e97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ea065a6e9bb4e4a9801a2399cd8d430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ee8a2bf38bb4a0eb3dfb7ed5f9e8565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7e48f9232c542a083af60e7d0e35098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silvsilvsilv/androidRESTAPI/blob/main/XLM_RoBERTa_Large_with_LoRA_fine_tuning_injected_noise_and_partial_unfreezing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf2dMTi7llZm",
        "outputId": "4c7e721e-b657-44e2-94ee-fe58ea3b39c3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XLM-RoBERTa + LoRA Fine-tuning: 5-Trial Hate Speech Detection\n",
        "# Multilingual (English, Tagalog, Cebuano) Binary Classification\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: Setup and Installation\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q transformers datasets accelerate peft evaluate scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    AutoModelForMaskedLM\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset\n",
        "import json"
      ],
      "metadata": {
        "id": "mYj0om2xnsM6"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 2: Configuration\n",
        "# ============================================================================\n",
        "\n",
        "# Training hyperparameters\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 7e-5\n",
        "FP16 = False  # Changed to False to avoid gradient scaling issues\n",
        "BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()  # Use BF16 if available\n",
        "LORA_R = 64\n",
        "LORA_ALPHA = 256\n",
        "LORA_DROPOUT = 0.1\n",
        "TARGET_MODULES = [\"query\", \"value\", \"dense\", \"output.dense\"]\n",
        "NUM_TRIALS = 5\n",
        "SEEDS = [42, 123, 2025, 7, 99]  # Custom seeds for each trial\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"xlm-roberta-large\"\n",
        "MAX_LENGTH = 128\n",
        "NUM_LABELS = 2\n",
        "\n",
        "# Set base paths (MODIFY THESE TO YOUR GOOGLE DRIVE PATHS)\n",
        "BASE_DIR = \"/content/drive/MyDrive/hate_speech_detection_cleaned\"\n",
        "DATA_DIR = f\"/content/drive/MyDrive/Machine_Learning/dataset\"\n",
        "OUTPUT_DIR = f\"{BASE_DIR}/models\"\n",
        "RESULTS_DIR = f\"{BASE_DIR}/results\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# Data file paths\n",
        "TRAIN_FILE = f\"{DATA_DIR}/unique_train_dataset_cleaned (1).csv\"\n",
        "VAL_FILE = f\"{DATA_DIR}/unique_validation_dataset_cleaned (1).csv\"\n",
        "TEST_FILE = f\"{DATA_DIR}/unique_test_dataset_cleaned (1).csv\"\n",
        "\n",
        "print(\"‚úì Configuration loaded\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Trials: {NUM_TRIALS}\")\n",
        "print(f\"  Seeds: {SEEDS}\")\n",
        "print(f\"  Epochs per trial: {EPOCHS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  LoRA r={LORA_R}, alpha={LORA_ALPHA}\")\n",
        "print(f\"  FP16: {FP16}, BF16: {BF16}\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "xJ6Wx-CEns0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc85c411-8475-4127-ac35-d98a817e2ed3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Configuration loaded\n",
            "  Model: xlm-roberta-large\n",
            "  Trials: 5\n",
            "  Seeds: [42, 123, 2025, 7, 99]\n",
            "  Epochs per trial: 5\n",
            "  Batch size: 8\n",
            "  Learning rate: 7e-05\n",
            "  LoRA r=64, alpha=256\n",
            "  FP16: False, BF16: True\n",
            "  Output directory: /content/drive/MyDrive/hate_speech_detection_cleaned/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 3: Utility Functions\n",
        "# ============================================================================\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"Set random seed for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def load_datasets():\n",
        "    \"\"\"Load and prepare datasets\"\"\"\n",
        "    print(\"\\nüìÇ Loading datasets...\")\n",
        "\n",
        "    train_df = pd.read_csv(TRAIN_FILE)\n",
        "    val_df = pd.read_csv(VAL_FILE)\n",
        "    test_df = pd.read_csv(TEST_FILE)\n",
        "\n",
        "    print(f\"  Train: {len(train_df)} samples\")\n",
        "    print(f\"  Validation: {len(val_df)} samples\")\n",
        "    print(f\"  Test: {len(test_df)} samples\")\n",
        "\n",
        "    # Convert to HF Dataset format\n",
        "    train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
        "    val_dataset = Dataset.from_pandas(val_df[['text', 'label']])\n",
        "    test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, test_df\n",
        "\n",
        "def tokenize_function(examples, tokenizer):\n",
        "    \"\"\"Tokenize text data\"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute evaluation metrics\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    micro_f1 = precision_recall_fscore_support(\n",
        "        labels, predictions, average='micro', zero_division=0\n",
        "    )[2]\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'macro_f1': f1,\n",
        "        'micro_f1': micro_f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Wlb4E9_unvMD"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Function: create_lora_model_with_noise_unfreeze\n",
        "# Description:\n",
        "#   Loads XLM-RoBERTa with LoRA fine-tuning, injects controlled noise,\n",
        "#   unfreezes top layers, and returns a ready-to-train model.\n",
        "# ==============================================================\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def create_lora_model_with_noise_unfreeze(model_name, num_labels):\n",
        "    # 1Ô∏è‚É£ Load base model (float32 for FP16 stability)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    # 2Ô∏è‚É£ Configure LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        target_modules=TARGET_MODULES,\n",
        "        bias=\"none\"\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # 3Ô∏è‚É£ Inject small Gaussian noise into embeddings\n",
        "    def add_embedding_noise(model, noise_std=0.005):\n",
        "        orig_forward = model.roberta.embeddings.forward\n",
        "        def noisy_forward(*args, **kwargs):\n",
        "            embeddings = orig_forward(*args, **kwargs)\n",
        "            if model.training:\n",
        "                noise = torch.randn_like(embeddings) * noise_std\n",
        "                embeddings = embeddings + noise\n",
        "            return embeddings\n",
        "        model.roberta.embeddings.forward = noisy_forward\n",
        "\n",
        "    add_embedding_noise(model, noise_std=0.005)\n",
        "\n",
        "    # 4Ô∏è‚É£ Unfreeze top-2 transformer layers\n",
        "    unfrozen_layers = [\"encoder.layer.22\", \"encoder.layer.23\"]\n",
        "    for name, param in model.named_parameters():\n",
        "        if any(layer in name for layer in unfrozen_layers):\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # 5Ô∏è‚É£ Slightly increase LoRA dropout\n",
        "    for name, module in model.named_modules():\n",
        "        if \"lora\" in name.lower():\n",
        "            if hasattr(module, \"dropout\"):\n",
        "                module.dropout.p = 0.2\n",
        "\n",
        "    # 6Ô∏è‚É£ Verify\n",
        "    model.print_trainable_parameters()\n",
        "    print(\"‚úÖ Added noise to embeddings (std=0.02)\")\n",
        "    print(\"‚úÖ Unfrozen layers:\", unfrozen_layers)\n",
        "    print(\"‚úÖ Increased LoRA dropout to 0.2\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "yXc4GFryzyUR"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 4: Training Function\n",
        "# ============================================================================\n",
        "\n",
        "def train_single_trial(trial_num, train_dataset, val_dataset, tokenizer):\n",
        "    \"\"\"Train a single trial with specified seed\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üöÄ TRIAL {trial_num}/{NUM_TRIALS}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Set seed for this trial using predefined seeds\n",
        "    seed = SEEDS[trial_num - 1]\n",
        "    set_seed(seed)\n",
        "    print(f\"  Seed: {seed}\")\n",
        "\n",
        "    # Create model with LoRA\n",
        "    model = create_lora_model_with_noise_unfreeze(MODEL_NAME, NUM_LABELS)\n",
        "\n",
        "    # Define output directory for this trial\n",
        "    trial_output_dir = f\"{OUTPUT_DIR}/trial_{trial_num}\"\n",
        "    os.makedirs(trial_output_dir, exist_ok=True)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=trial_output_dir,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"macro_f1\",\n",
        "        greater_is_better=True,\n",
        "        fp16=False,  # Disabled FP16\n",
        "        bf16=BF16,  # Use BF16 if available\n",
        "        logging_dir=f\"{trial_output_dir}/logs\",\n",
        "        logging_steps=50,\n",
        "        seed=seed,\n",
        "        report_to=\"none\",\n",
        "        save_total_limit=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        dataloader_pin_memory=False,  # Additional stability\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        processing_class=tokenizer,  # Updated from 'tokenizer' parameter\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n  Training started...\")\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    # Save final model\n",
        "    trainer.save_model(trial_output_dir)\n",
        "    tokenizer.save_pretrained(trial_output_dir)\n",
        "\n",
        "    # Get validation metrics\n",
        "    val_metrics = trainer.evaluate()\n",
        "\n",
        "    print(f\"\\n  ‚úì Trial {trial_num} completed\")\n",
        "    print(f\"    Validation Macro F1: {val_metrics['eval_macro_f1']:.4f}\")\n",
        "    print(f\"    Validation Precision: {val_metrics['eval_precision']:.4f}\")\n",
        "    print(f\"    Validation Recall: {val_metrics['eval_recall']:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'trial': trial_num,\n",
        "        'seed': seed,\n",
        "        'val_macro_f1': val_metrics['eval_macro_f1'],\n",
        "        'val_precision': val_metrics['eval_precision'],\n",
        "        'val_recall': val_metrics['eval_recall'],\n",
        "        'val_micro_f1': val_metrics['eval_micro_f1'],\n",
        "        'model_path': trial_output_dir\n",
        "    }"
      ],
      "metadata": {
        "id": "JDEqCk-SnxkD"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 5: Testing Function\n",
        "# ============================================================================\n",
        "\n",
        "def test_model(model_path, test_dataset, tokenizer, test_df):\n",
        "    \"\"\"Test a trained model and return detailed metrics\"\"\"\n",
        "    print(f\"\\n  Loading model from: {model_path}\")\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_path,\n",
        "        local_files_only=True\n",
        "    )\n",
        "    model.eval()\n",
        "    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Prepare test data\n",
        "    def tokenize_batch(batch):\n",
        "        return tokenizer(\n",
        "            batch['text'],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "    # Make predictions\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(test_dataset), BATCH_SIZE):\n",
        "            batch = test_dataset[i:i+BATCH_SIZE]\n",
        "            inputs = tokenize_batch(batch)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(batch['label'])\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    micro_f1 = precision_recall_fscore_support(\n",
        "        all_labels, all_predictions, average='micro', zero_division=0\n",
        "    )[2]\n",
        "\n",
        "    # Confusion matrix (flattened)\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    cm_flat = cm.flatten().tolist()\n",
        "\n",
        "    return {\n",
        "        'test_precision': precision,\n",
        "        'test_recall': recall,\n",
        "        'test_macro_f1': f1,\n",
        "        'test_micro_f1': micro_f1,\n",
        "        'confusion_matrix': cm_flat,\n",
        "        'predictions': all_predictions,\n",
        "        'labels': all_labels\n",
        "    }\n"
      ],
      "metadata": {
        "id": "2pCArBANn3dK"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fca6ecd",
        "outputId": "93285d31-20e5-431f-dae2-71dcd94a31bd"
      },
      "source": [
        "# Install the huggingface_hub library\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "# Import and login\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the token from Colab secrets and login\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(token=hf_token)\n",
        "    print(\"‚úì Successfully logged in to Hugging Face Hub.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error logging in to Hugging Face Hub: {e}\")\n",
        "    print(\"Please ensure you have added your Hugging Face token to Colab secrets with the name 'HF_TOKEN'.\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Successfully logged in to Hugging Face Hub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "370dbd0f"
      },
      "source": [
        "**Troubleshooting Hugging Face Authentication**\n",
        "\n",
        "The error indicates an authorization issue when accessing the model `xlm-roberta-large` from the Hugging Face Hub. This often occurs if the model is gated or requires authentication.\n",
        "\n",
        "To resolve this, you need to provide your Hugging Face API token. You can generate a token in your Hugging Face account settings (under \"Access Tokens\").\n",
        "\n",
        "For secure storage, it's recommended to save your token in Colab's Secrets Manager (the key icon on the left sidebar) with a name like `HF_TOKEN`.\n",
        "\n",
        "The following cell will install the `huggingface_hub` library and log you in using the token from Colab secrets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 6: Main Execution\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"XLM-RoBERTa + LoRA: Multi-Trial Hate Speech Detection\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset, val_dataset, test_dataset, test_df = load_datasets()\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"\\nüî§ Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # Tokenize datasets\n",
        "    print(\"  Tokenizing train and validation datasets...\")\n",
        "    train_dataset = train_dataset.map(\n",
        "        lambda x: tokenize_function(x, tokenizer),\n",
        "        batched=True\n",
        "    )\n",
        "    val_dataset = val_dataset.map(\n",
        "        lambda x: tokenize_function(x, tokenizer),\n",
        "        batched=True\n",
        "    )\n",
        "    # Test dataset tokenization will happen in the test_model function\n",
        "    # test_dataset = test_dataset.map(\n",
        "    #     lambda x: tokenize_function(x, tokenizer),\n",
        "    #     batched=True\n",
        "    # )\n",
        "\n",
        "    # Set format for PyTorch\n",
        "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    # test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label']) # Format later in test_model\n",
        "\n",
        "    # Train all trials\n",
        "    all_trial_results = []\n",
        "\n",
        "    for trial_num in range(1, NUM_TRIALS + 1):\n",
        "        trial_result = train_single_trial(\n",
        "            trial_num,\n",
        "            train_dataset,\n",
        "            val_dataset,\n",
        "            tokenizer\n",
        "        )\n",
        "        all_trial_results.append(trial_result)\n",
        "\n",
        "    # Find best model based on validation macro F1\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä TRIAL SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    best_trial = max(all_trial_results, key=lambda x: x['val_macro_f1'])\n",
        "\n",
        "    for result in all_trial_results:\n",
        "        is_best = \"‚≠ê BEST\" if result['trial'] == best_trial['trial'] else \"\"\n",
        "        print(f\"Trial {result['trial']}: Val Macro F1 = {result['val_macro_f1']:.4f} {is_best}\")\n",
        "\n",
        "    print(f\"\\nüèÜ Best Model: Trial {best_trial['trial']}\")\n",
        "    print(f\"   Validation Macro F1: {best_trial['val_macro_f1']:.4f}\")\n",
        "    print(f\"   Model Path: {best_trial['model_path']}\")\n",
        "\n",
        "    # Test all models and save results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üß™ TESTING ALL MODELS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    final_results = []\n",
        "\n",
        "    for trial_result in all_trial_results:\n",
        "        print(f\"\\nTesting Trial {trial_result['trial']}...\")\n",
        "\n",
        "        test_metrics = test_model(\n",
        "            trial_result['model_path'],\n",
        "            test_dataset, # Pass the original test_dataset\n",
        "            tokenizer,\n",
        "            test_df\n",
        "        )\n",
        "\n",
        "        # Combine trial and test results\n",
        "        combined_result = {\n",
        "            'trial': trial_result['trial'],\n",
        "            'seed': trial_result['seed'],\n",
        "            'val_precision': trial_result['val_precision'],\n",
        "            'val_recall': trial_result['val_recall'],\n",
        "            'val_macro_f1': trial_result['val_macro_f1'],\n",
        "            'val_micro_f1': trial_result['val_micro_f1'],\n",
        "            'test_precision': test_metrics['test_precision'],\n",
        "            'test_recall': test_metrics['test_recall'],\n",
        "            'test_macro_f1': test_metrics['test_macro_f1'],\n",
        "            'test_micro_f1': test_metrics['test_micro_f1'],\n",
        "            'cm_tn': test_metrics['confusion_matrix'][0],\n",
        "            'cm_fp': test_metrics['confusion_matrix'][1],\n",
        "            'cm_fn': test_metrics['confusion_matrix'][2],\n",
        "            'cm_tp': test_metrics['confusion_matrix'][3],\n",
        "            'is_best_model': trial_result['trial'] == best_trial['trial']\n",
        "        }\n",
        "\n",
        "        final_results.append(combined_result)\n",
        "\n",
        "        print(f\"  Test Macro F1: {test_metrics['test_macro_f1']:.4f}\")\n",
        "        print(f\"  Test Precision: {test_metrics['test_precision']:.4f}\")\n",
        "        print(f\"  Test Recall: {test_metrics['test_recall']:.4f}\")\n",
        "\n",
        "    # Save all results to CSV\n",
        "    results_df = pd.DataFrame(final_results)\n",
        "    results_file = f\"{RESULTS_DIR}/all_trials_results.csv\"\n",
        "    results_df.to_csv(results_file, index=False)\n",
        "    print(f\"\\n‚úì Results saved to: {results_file}\")\n",
        "\n",
        "    # Copy best model to dedicated directory\n",
        "    best_model_dir = f\"{OUTPUT_DIR}/best_model\"\n",
        "    print(f\"\\nüì¶ Copying best model to: {best_model_dir}\")\n",
        "\n",
        "    import shutil\n",
        "    if os.path.exists(best_model_dir):\n",
        "        shutil.rmtree(best_model_dir)\n",
        "    shutil.copytree(best_trial['model_path'], best_model_dir)\n",
        "\n",
        "    # Save best model info\n",
        "    best_model_info = {\n",
        "        'trial': best_trial['trial'],\n",
        "        'seed': best_trial['seed'],\n",
        "        'val_macro_f1': best_trial['val_macro_f1'],\n",
        "        'test_macro_f1': results_df[results_df['is_best_model'] == True]['test_macro_f1'].values[0],\n",
        "        'model_path': best_model_dir,\n",
        "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "\n",
        "    with open(f\"{best_model_dir}/best_model_info.json\", 'w') as f:\n",
        "        json.dump(best_model_info, f, indent=2)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ TRAINING COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total trials: {NUM_TRIALS}\")\n",
        "    print(f\"Best trial: {best_trial['trial']}\")\n",
        "    print(f\"Best model saved to: {best_model_dir}\")\n",
        "    print(f\"All results saved to: {results_file}\")\n",
        "    print(\"\\nBest Model Performance:\")\n",
        "    best_result = results_df[results_df['is_best_model'] == True].iloc[0]\n",
        "    print(f\"  Validation Macro F1: {best_result['val_macro_f1']:.4f}\")\n",
        "    print(f\"  Test Macro F1: {best_result['test_macro_f1']:.4f}\")\n",
        "    print(f\"  Test Precision: {best_result['test_precision']:.4f}\")\n",
        "    print(f\"  Test Recall: {best_result['test_recall']:.4f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Eu8-GRn4n3_L"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "MCXRdrbFoiS1",
        "outputId": "ecece28f-be0a-4806-fdc6-343216845647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625,
          "referenced_widgets": [
            "8aa66e2e26ca4e06b8f9d17e39ca6ae3",
            "ce1b8f0704874b20b483b22924365a99",
            "61294b9a721d44949e1e0a6ac6378364",
            "a8b7f6db03694f788888e1bd3c6b9abc",
            "f3fcbac128404fe5ab997cca6e719e47",
            "9ce7d679a8fc41d9b2b1d871bfa5434f",
            "4a89766033bf45278e894d0bbd2110b9",
            "cd362f1114664dc5834c950b36d7548d",
            "27c7f409b51f489a996a90ab7e13c887",
            "39ffa8fc49054253aba8decbb744a301",
            "fd49d890c7494dce932a168fb19218c7",
            "e05667d7d1f14fa4bd042b0a0fcb9b9d",
            "c941d1c5fb6d4cdd9fbbf4b8e613e1ab",
            "d4714a97f35d4b4389de1210a48498e7",
            "7f7deabc545e498594c7cab4a57bfaed",
            "fb6e6f47dc3a40c2b44f9d031392c344",
            "9cd911ef29194f1bbbfda7be7699959b",
            "dce4006d90ad4f38bd1f46a7fb249816",
            "17a6cfe00751426590e946b307374e97",
            "8ea065a6e9bb4e4a9801a2399cd8d430",
            "8ee8a2bf38bb4a0eb3dfb7ed5f9e8565",
            "b7e48f9232c542a083af60e7d0e35098"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "XLM-RoBERTa + LoRA: Multi-Trial Hate Speech Detection\n",
            "======================================================================\n",
            "\n",
            "üìÇ Loading datasets...\n",
            "  Train: 21767 samples\n",
            "  Validation: 2800 samples\n",
            "  Test: 2808 samples\n",
            "\n",
            "üî§ Loading tokenizer...\n",
            "  Tokenizing train and validation datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/21767 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8aa66e2e26ca4e06b8f9d17e39ca6ae3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e05667d7d1f14fa4bd042b0a0fcb9b9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ TRIAL 1/5\n",
            "======================================================================\n",
            "  Seed: 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 51,409,922 || all params: 586,109,956 || trainable%: 8.7714\n",
            "‚úÖ Added noise to embeddings (std=0.02)\n",
            "‚úÖ Unfrozen layers: ['encoder.layer.22', 'encoder.layer.23']\n",
            "‚úÖ Increased LoRA dropout to 0.2\n",
            "\n",
            "  Training started...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3792' max='13605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3792/13605 10:43 < 27:45, 5.89 it/s, Epoch 1.39/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Micro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.696400</td>\n",
              "      <td>0.694768</td>\n",
              "      <td>0.252679</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.335706</td>\n",
              "      <td>0.505357</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}